# PatchCore Flask Service

**Package Damage Detection using PatchCore Anomaly Detection**

> âœ… EXACT copy of notebook logic - ZERO modifications  
> âœ… Production-ready Flask service  
> âœ… Simple setup & deployment  

---

## ğŸš€ Quick Start (3 Steps)

### Step 1: Setup Environment

**Using PowerShell:**
```powershell
.\setup.ps1
```

**Using Command Prompt:**
```cmd
setup.bat
```

**Or manually:**
```powershell
python -m venv venv
.\venv\Scripts\activate
pip install -r requirements.txt
```

### Step 2: Run Service

```powershell
python app.py
```

Output:
```
======================================================================
PatchCore Flask Service - Ready for Hackathon Demo
======================================================================
Endpoints:
  GET  /              - Service info
  GET  /health        - Health check
  POST /load_reference - Upload normal image
  POST /infer         - Get overlay PNG
  POST /infer_json    - Get JSON result
======================================================================

Starting Flask server on http://127.0.0.1:5000
======================================================================
```

### Step 3: Test It

```powershell
python test_service.py
```

---

## ğŸ“ Project Structure

```
patchcore_service/
â”‚
â”œâ”€â”€ app.py                  â† Flask service (EXACT notebook logic)
â”œâ”€â”€ requirements.txt        â† Dependencies (notebook-compatible)
â”œâ”€â”€ setup.ps1              â† Setup script (PowerShell)
â”œâ”€â”€ setup.bat              â† Setup script (CMD)
â”œâ”€â”€ test_service.py        â† Automated test suite
â”œâ”€â”€ README.md              â† This file
â”‚
â”œâ”€â”€ models/                â† Memory bank storage (auto-created)
â”œâ”€â”€ static/
â”‚   â””â”€â”€ outputs/           â† Saved result images
â”‚
â””â”€â”€ venv/                  â† Virtual environment (after setup)
```

---

## ğŸ”Œ API Usage

### 1ï¸âƒ£ Health Check

```bash
curl http://127.0.0.1:5000/health
```

**Response:**
```json
{
  "status": "healthy",
  "model": "PatchCore with WideResNet-50-2",
  "memory_bank_loaded": false,
  "device": "cpu"
}
```

### 2ï¸âƒ£ Load Normal Reference

Upload a pristine package image (one-time setup):

```bash
curl -X POST http://127.0.0.1:5000/load_reference \
  -F "image=@normal_package.jpg"
```

**Response:**
```json
{
  "status": "success",
  "message": "Memory bank built successfully",
  "patches": 784,
  "feature_dim": 1536
}
```

### 3ï¸âƒ£ Inference - Get Overlay PNG

```bash
curl -X POST http://127.0.0.1:5000/infer \
  -F "image=@test_package.jpg" \
  -o result.png
```

Returns: PNG image with anomaly heatmap overlay

### 4ï¸âƒ£ Inference - Get JSON Result

```bash
curl -X POST http://127.0.0.1:5000/infer_json \
  -F "image=@test_package.jpg"
```

**Response:**
```json
{
  "status": "success",
  "damage_percentage": 12.34,
  "anomaly_detected": true,
  "output_image": "result_12345.png",
  "threshold": 0.5
}
```

---

## ğŸ Python Client Example

```python
import requests

# 1. Load reference image
with open('normal_package.jpg', 'rb') as f:
    response = requests.post(
        'http://127.0.0.1:5000/load_reference',
        files={'image': f}
    )
print(response.json())

# 2. Run inference (PNG output)
with open('test_package.jpg', 'rb') as f:
    response = requests.post(
        'http://127.0.0.1:5000/infer',
        files={'image': f}
    )

# Save result
with open('result.png', 'wb') as f:
    f.write(response.content)

# 3. Run inference (JSON output)
with open('test_package.jpg', 'rb') as f:
    response = requests.post(
        'http://127.0.0.1:5000/infer_json',
        files={'image': f}
    )
data = response.json()
print(f"Damage: {data['damage_percentage']:.2f}%")
```

---

## ğŸ”¬ Notebook Fidelity

### Architecture (EXACT)
- **Backbone:** WideResNet-50-2 (pretrained ImageNet)
- **Feature Layers:** Layer 2 (512 channels) + Layer 3 (1024 channels)
- **Feature Extraction:** Concatenate layer2 + upsampled layer3
- **Patches:** 28Ã—28 = 784 patches per image
- **Feature Dim:** 1536 (512 + 1024)

### Parameters (EXACT)
- **Input Size:** 224Ã—224
- **Normalization:** ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
- **k-NN Neighbors:** 9
- **Distance Metric:** Euclidean
- **Anomaly Threshold:** 0.5
- **Heatmap Colormap:** COLORMAP_JET
- **Overlay Blend:** 0.5 weight for original + 0.5 for heatmap

### Pipeline (EXACT)
```
Input â†’ RGB Conversion â†’ Resize 224Ã—224 â†’ ToTensor â†’ Normalize
  â†“
WideResNet-50-2 (layer2: 28Ã—28, layer3: 14Ã—14)
  â†“
Upsample layer3 to 28Ã—28 (bilinear)
  â†“
Concatenate â†’ 784 patches Ã— 1536 dims
  â†“
k-NN distance to memory bank (k=9)
  â†“
Mean distance â†’ Anomaly scores
  â†“
Reshape 28Ã—28 â†’ Upsample 224Ã—224
  â†“
Normalize [0,1] â†’ Apply COLORMAP_JET â†’ Blend with original
  â†“
Output: Overlay PNG
```

---

## âš™ï¸ Configuration

### GPU Support

Change line in `app.py`:

```python
# CPU (default)
device = torch.device('cpu')

# GPU
device = torch.device('cuda')
```

### Port Change

```python
app.run(host='127.0.0.1', port=8080, debug=False)
```

### Max File Size

```python
app.config['MAX_CONTENT_LENGTH'] = 32 * 1024 * 1024  # 32MB
```

---

## ğŸ› Troubleshooting

### "Memory bank not loaded"
**Solution:** Call `/load_reference` before `/infer`

### Slow inference
**Cause:** CPU processing is slow for WideResNet-50-2  
**Solution:** Use GPU (change `device` to `'cuda'`)

### Import errors
```powershell
pip install flask torch torchvision opencv-python pillow scikit-learn
```

### Port already in use
**Solution:** Change port in `app.py` or kill existing process

---

## ğŸ“¦ Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| flask | 3.0.0 | Web framework |
| torch | 2.1.0 | Deep learning |
| torchvision | 0.16.0 | Pretrained models |
| opencv-python | 4.8.1.78 | Image processing |
| pillow | 10.1.0 | Image I/O |
| scikit-learn | 1.3.2 | k-NN index |
| numpy | 1.24.3 | Array operations |

---

## ğŸš¢ Production Deployment (Optional)

### Using Gunicorn

```powershell
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app --timeout 120
```

### Using Docker

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY app.py .
COPY models/ models/
COPY static/ static/

EXPOSE 5000
CMD ["python", "app.py"]
```

Build & run:
```bash
docker build -t patchcore-service .
docker run -p 5000:5000 patchcore-service
```

---

## âœ… Hackathon Demo Checklist

- âœ… Model loads successfully
- âœ… Reference image builds memory bank
- âœ… Inference returns overlay image
- âœ… All logic matches notebook exactly
- âœ… No architectural changes
- âœ… No parameter modifications
- âœ… CPU-compatible
- âœ… Single-file service
- âœ… Simple REST API
- âœ… Test suite included
- âœ… Easy setup (3 commands)

---

## ğŸ“ Notes

- **No modifications:** All notebook logic preserved exactly
- **CPU-only default:** Works on any machine (use GPU for speed)
- **Memory bank:** Loaded once, persists for all requests
- **Image formats:** Auto-converts RGBAâ†’RGB
- **Threshold:** Fixed at 0.5 (same as notebook)

---

## ğŸ¯ Next Steps

1. **Test locally:** Run `python test_service.py`
2. **Use real images:** Replace test images with actual package photos
3. **Deploy:** Use gunicorn/Docker for production
4. **Integrate:** Call from frontend/mobile app
5. **Monitor:** Add logging/metrics if needed

---

## ğŸ“„ License

Same as original notebook

---

**Questions?** Check the test script or API documentation above.
